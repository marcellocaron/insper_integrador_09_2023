---
title: "modelagem_final"
author: "Murilo Cechin"
date: "2023-09-12"
output: html_document
---

# Modelagem Projeto Integrador

Arquivo de modelagem da atividade integradora.

O grupo selecionou os seguintes algoritimos para realizar a modelagem de classificação:


- Random Forest
- Regressão Logistica
- Ridge
- XGboost


## Bibliotecas utilizadas no projeto

```{r}
library(tidyverse)
library(rsample)
library(glmnet)
library(ranger)
library(pROC)
library(class)
library(caret)
```

## Importando arquivo

```{r}
df_final = read.csv("D:/Usuário/Documents/MeusProjetos/insper_integrador_09_2023/projeto_final/df_final.csv")

head(df_final, 10)
```

Antes de proceguir com a modelagem é necessario retirar algumas colunas deste dataframe. Como a coluna X, que representa o index do pré-processamento em python e a coluna comp_id que apenas fornece o código da empresa.

```{r}
df_final$X <- NULL
```

```{r}
df_final$comp_id <- NULL
```

```{r}
df_final
```



### Colunas com log 
  Todas as colunas nomeadas com "_log" no final foram processadas utilizando a escala logaritimica de acordo com a seguinte regra:

  $$ \text{"coluna"\_log}(x) = 
\begin{cases} 
\log(x) & \text{se } x > 0 \\
0 & \text{se } x = 0 \\
\log(|x|) * -1 & \text{se } x \leq 0 

\end{cases} $$

Caso seja necessario realizar a interpretação do código, rodar a função abaixo:

```{r}
back_log <- function(x) {
  if (x > 0) {
    return(exp(x))
  } else if (x == 0) {
    return(0)
  } else {
    y <- -1 * x
    return(exp(y) * -1)
  }
}
```

Aplicando na coluna profit_loss_year_log para demonstrar:

```{r}
# Antes de aplicar a função
head(df_final$profit_loss_year_log, 10)
```

```{r}
# Aplicando a função
head(sapply(df_final$profit_loss_year_log, back_log), 10)

```

### Conjunto de teste e treinamento

Separando os conjuntos de teste e treinamento.

Antes, vamos analisar se o cojunto de dados esta desbalanceado.

```{r}
# Analise das frequencia

df_final %>%
  group_by(target) %>%
  summarise(freq_absoluta = n()) %>%
  mutate(freq_relativa = (freq_absoluta / sum(freq_absoluta))*100)

```

Percebemos que a base de dados apresenta um desbalanceamento significativo, com somente 4% das empresas sendo classificadas como inativas. Devido a essa disparidade, a métrica de acurácia pode não oferecer uma visão completa do desempenho do modelo. Por isso, optaremos pela Sensibilidade como métrica principal, pois ela indica a proporção de verdadeiros positivos que foram corretamente identificados.

Para dividir os conjuntos de treinamento e teste, vamos adotar uma amostragem estratificada, de forma a preservar a proporção da variável alvo.


```{r}
# Configurando semente aleatoria
set.seed(123)

# Aplicando função factor

df_final$target <- factor(df_final$target)

splits <- initial_split(df_final, prop = .8, strata="target")

conj_treinamento <- training(splits)
conj_teste <- testing(splits)
```


```{r}
# Conferindo proporção do conjunto de teste
conj_teste %>%
  group_by(target) %>%
  summarise(freq_absoluta = n()) %>%
  mutate(freq_relativa = (freq_absoluta / sum(freq_absoluta))*100)
```


```{r}
# Conferindo conjunto de treinamento
conj_treinamento %>%
  group_by(target) %>%
  summarise(freq_absoluta = n()) %>%
  mutate(freq_relativa = (freq_absoluta / sum(freq_absoluta))*100)
```

Como pode ser obsevado , a estratificação funcionou.

### Criando Tabela de comparação

Além da métrica de sensibilidade, também será analisada a AUC para obter uma visão geral do desempenho do modelo.


```{r}
tb_metricas <- tibble(
  modelo = c("Random Forest","Regressão Logistica", "Ridge", "XGboost"),
  AUC = c(NA, NA, NA, NA),
)

tb_metricas
```



### Random Forest

Criando modelagem com algoritmo random forest.

```{r}
random <- ranger(target ~., data=conj_treinamento)

print(random)
```

```{r}
teste_random = predict(random, conj_teste)

probs_numeric <- as.numeric(as.character(teste_random$predictions)) - 1
labels_numeric <- as.numeric(as.character(conj_teste$target))

roc_obj <- roc(labels_numeric, probs_numeric)
auc_random <- auc(roc_obj)

auc_random
```
```{r}
tb_metricas
```


Otimizando os hiperparametros de n_trees e Mtry.

```{r}
# Buscando um número otimo de n_trees
random_ntree <- tibble(n_arvores = c(1:15, seq(25, 300, 25)), erro = NA)

random_ntree <- random_ntree %>%
  mutate(erro = map_dbl(n_arvores, ~ranger(target ~ ., num.trees = .x, data=conj_treinamento)$prediction.error))

random_ntree %>% 
  ggplot(aes(n_arvores, erro)) + 
    geom_line(color = "#5B5FFF", size = 1.2) + 
    labs(x = "Número de Árvores", y = "Erro de Classificação (OOB)") + 
    theme_bw()

```


Analisando o gráfico gerado, iremos testar entre 50 e 200 árvores.

```{r}
# Otimizando o mtry e o número de árvores.
resultados <- crossing(mtry = c(4, 8, 10, 15, 20, 25, 30), 
                       n_arvores = c(1, 5, 10, seq(50, 200, 25)))

ajusta <- function(mtry, n_arvores) {
   random <- ranger(target ~ ., num.trees = n_arvores, mtry = mtry, data = conj_treinamento)
   return(random$prediction.error)
}

resultados <- resultados %>% 
  mutate(erro = map2_dbl(mtry, n_arvores, ajusta))


head(resultados)
```


Plotando os resultados
```{r}
resultados %>% 
  mutate(mtry = factor(mtry)) %>% 
  ggplot(aes(n_arvores, erro, group = mtry, color = mtry)) + 
    geom_line( size = 1.2) +
    labs(x = "Número de Árvores", y = "Erro de Classificação (OOB)") +
    theme_bw()
```
Analisando resultados com os menores erros:
```{r}
resultados %>%
  arrange(erro)
```
A árvore otimizada será com mtry=4 e n_arvores = 100.
```{r}
conj_treinamento$target <- as.factor(conj_treinamento$target)

random_final <- ranger(target ~., data=conj_treinamento, num.trees = 100, mtry=4)

print(random_final)
```
Realizando predição no conjunto de teste

```{r}
# Realizando predições no conjunto de teste
predicoes_random <- predict(random_final, data = conj_teste)

# Probabilidades da classe positiva 
probs_random <- predicoes_random$predictions

# Matriz de confusão e sensibilidade
confusion_random <- confusionMatrix(data=probs_random, reference = as.factor(conj_teste$target), positive='1')

confusion_random
```
Obtendo a AUC:
```{r}
probs_numeric <- as.numeric(as.character(predicoes_random$predictions)) - 1
labels_numeric <- as.numeric(as.character(conj_teste$target))

roc_obj <- roc(labels_numeric, probs_numeric)
auc_random <- auc(roc_obj)

auc_random
```


Adicionando resultados no tibble de comparação


```{r}
tb_metricas$AUC[tb_metricas$modelo == "Random Forest"] = auc_random
tb_metricas
```

### Regressão Logistica

Para a realizar a regressão logistica, primeiro será necessario criar as matrizes de entrada.

```{r}
reg_logistica <- glm(target ~ ., data = conj_treinamento, family = "binomial")

reg_logistica
```

Cálculando as probabilidades

```{r}
prob_log <- predict(reg_logistica, conj_teste, type = "response")
```

Cálculando métrica AUC

```{r}
roc_log <- roc(conj_teste$target, prob_log)

roc_log
```

Adicionando AUC para a tabela de resultados

```{r}
tb_metricas$AUC[tb_metricas$modelo == "Regressão Logistica"] = roc_log$auc

tb_metricas
```



### Ridge

Para utilizar o pacote glmnet, primeiro é necessario realizar um tratamento nos conjuntos de teste e treinamento.

```{r}
conj_treinamento_matrix <- model.matrix(target~., data = conj_treinamento)[, -1]

conj_teste_matrix <- model.matrix(target~. , data = conj_teste)[, -1]
```

Treinando o modelo

```{r}
ridge <- glmnet(conj_treinamento_matrix, conj_treinamento$target, alpha = 0, family = "binomial")

summary(ridge)
```

Realizando validação cruzada para otimizar lambda

```{r}
cv.ridge <- cv.glmnet(conj_treinamento_matrix, conj_treinamento$target, alpha = 0, family = "binomial")
best_lambda <- cv.ridge$lambda.min 
```

```{r}
best_lambda
```

Ajustando modelo e realizando as predições
```{r}
predicoes_prob <- predict(ridge, s = best_lambda, newx = conj_teste_matrix, type = "response")

predicoes_class <- ifelse(predicoes_prob > 0.5, 1, 0) 
```

Cálculando a matriz de confusão
```{r}
table(Observed = conj_teste$target, Predicted = predicoes_class)
```
Obtendo a métrica AUC:

```{r}
roc_obj <- roc(conj_teste$target, predicoes_prob)  # Crie o objeto ROC
auc(roc_obj)  # Calcule a AUC
```
Adicionando resposta em tabela de comparação

```{r}
tb_metricas$AUC[tb_metricas$modelo == "Ridge"] = auc(roc_obj)

tb_metricas
```

















