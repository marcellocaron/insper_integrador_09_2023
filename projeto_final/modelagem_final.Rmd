---
title: "modelagem_final"
author: "Murilo Cechin, Marcello Caron, Mariana Aukar"
date: "2023-09-12"
output: html_document
---

# Modelagem Projeto Integrador

Arquivo de modelagem da atividade integradora.

O grupo selecionou os seguintes algoritimos para realizar a modelagem de classificação:


- Random Forest
- Regressão Logistica
- Ridge
- XGboost


```{r include=FALSE}
library(tidyverse)
library(rsample)
library(glmnet)
library(ranger)
library(pROC)
library(class)
library(caret)
library(Matrix)
library(xgboost)
library(yardstick)
```

## Importando arquivo

```{r}
df_final = read.csv('df_final.csv')

head(df_final, 10)
```

Antes de prosseguir com a modelagem é necessario retirar algumas colunas deste dataframe. Como a coluna X, que representa o index do pré-processamento em python e a coluna comp_id que apenas fornece o código da empresa.

```{r}
df_final$X <- NULL
```

```{r}
df_final$comp_id <- NULL
```

```{r}
head(df_final, 10)
```



### Colunas com log 
  Todas as colunas nomeadas com "_log" no final foram processadas utilizando a escala logaritimica de acordo com a seguinte regra:

  $$ \text{coluna_log}(x) = 
\begin{cases} 
\log(x) & \text{se } x > 0 \\
0 & \text{se } x = 0 \\
\log(|x|) * -1 & \text{se } x \leq 0 

\end{cases} $$

Caso seja necessario realizar a interpretação do código, rodar a função abaixo:

```{r}
back_log <- function(x) {
  if (x > 0) {
    return(exp(x))
  } else if (x == 0) {
    return(0)
  } else {
    y <- -1 * x
    return(exp(y) * -1)
  }
}
```

Aplicando na coluna profit_loss_year_log para demonstrar:

```{r}
# Antes de aplicar a função
head(df_final$profit_loss_year_log, 10)
```

```{r}
# Aplicando a função
head(sapply(df_final$profit_loss_year_log, back_log), 10)

```

### Conjunto de teste e treinamento

Separando os conjuntos de teste e treinamento.

Antes, vamos analisar se o cojunto de dados está desbalanceado.

```{r}
# Analise das frequencia

df_final %>%
  group_by(target) %>%
  summarise(freq_absoluta = n()) %>%
  mutate(freq_relativa = (freq_absoluta / sum(freq_absoluta))*100)

```

Percebemos que a base de dados apresenta um desbalanceamento significativo, com somente 14% das empresas sendo classificadas como inativas. Devido a essa disparidade, a métrica de acurácia pode não oferecer uma visão completa do desempenho do modelo. Por isso, optaremos pela Area Under the Curve (AUC) como métrica principal, pois ela faz um balanço entre Sensibilidade e Especificidade.

A AUC é menos sensível a desequilíbrio de classes do que algumas outras métricas, como a acurácia. Em problemas onde as classes estão desequilibradas, a AUC pode oferecer uma avaliação mais confiável do desempenho do modelo.

Para dividir os conjuntos de treinamento e teste, vamos adotar uma amostragem estratificada, de forma a preservar a proporção da variável alvo.


```{r}
# Configurando semente aleatoria
set.seed(123)

# Aplicando função factor

df_final$target <- factor(df_final$target)

splits <- initial_split(df_final, prop = .8, strata="target")

conj_treinamento <- training(splits)
conj_teste <- testing(splits)
```

Conferindo proporção do conjunto de teste

```{r}
conj_teste %>%
  group_by(target) %>%
  summarise(freq_absoluta = n()) %>%
  mutate(freq_relativa = (freq_absoluta / sum(freq_absoluta))*100)
```
Conferindo conjunto de treinamento

```{r}
conj_treinamento %>%
  group_by(target) %>%
  summarise(freq_absoluta = n()) %>%
  mutate(freq_relativa = (freq_absoluta / sum(freq_absoluta))*100)
```

Como pode ser obsevado , a estratificação funcionou.

### Criando Tabela de comparação


```{r}
tb_metricas <- tibble(
  modelo = c("Random Forest","Regressão Logistica", "Ridge", "XGboost"),
  AUC = c(NA, NA, NA, NA),
)

tb_metricas
```



### Random Forest

Criando modelagem com algoritmo random forest.

```{r}
random <- ranger(target ~ ., data=conj_treinamento, probability = TRUE)

print(random)
```
AUC para Random Forest pré-otimização de hiperparâmetros

```{r}
teste_random = predict(random, conj_teste)

probs_numeric <- as.numeric(as.character(teste_random$predictions[,2]))
labels_numeric <- as.numeric(as.character(conj_teste$target))

roc_obj <- roc(labels_numeric, probs_numeric)

auc_random <- auc(roc_obj)

auc_random
```

Otimizando os hiperparâmetros de n_trees e Mtry.

```{r}
# Buscando um número otimo de n_trees
random_ntree <- tibble(n_arvores = c(1:15, seq(25, 300, 25)), erro = NA)

random_ntree <- random_ntree %>%
  mutate(erro = map_dbl(n_arvores, ~ranger(target ~ ., num.trees = .x, data=conj_treinamento, probability = TRUE)$prediction.error))

random_ntree %>% 
  ggplot(aes(n_arvores, erro)) + 
    geom_line(color = "#5B5FFF", size = 1.2) + 
    labs(x = "Número de Árvores", y = "Erro de Classificação (OOB)") + 
    theme_bw()

```


Analisando o gráfico gerado, iremos testar entre 50 e 250 árvores.

```{r}
# Otimizando o mtry e o número de árvores.
resultados <- crossing(mtry = c(4, 8, 10, 15, 20, 25, 30), 
                       n_arvores = c(seq(50, 250, 25)))

ajusta <- function(mtry, n_arvores) {
   set.seed(123)
   random <- ranger(target ~ ., num.trees = n_arvores, mtry = mtry, data = conj_treinamento, probability = TRUE)
   return(random$prediction.error)
}

resultados <- resultados %>% 
  mutate(erro = map2_dbl(mtry, n_arvores, ajusta))


head(resultados)
```


Plotando os resultados
```{r}
resultados %>% 
  mutate(mtry = factor(mtry)) %>% 
  ggplot(aes(n_arvores, erro, group = mtry, color = mtry)) + 
    geom_line( size = 1.2) +
    labs(x = "Número de Árvores", y = "Erro de Classificação (OOB)") +
    theme_bw()
```
Analisando resultados com os menores erros:
```{r}
resultados %>%
  arrange(erro)
```
A árvore otimizada será com mtry = 4 e n_arvores = 250.
```{r}
conj_treinamento$target <- as.factor(conj_treinamento$target)

random_final <- ranger(target ~., data=conj_treinamento, num.trees = 250, mtry=4, probability = TRUE)

print(random_final)
```
Realizando predição no conjunto de teste

```{r}
# Realizando predições no conjunto de teste
predicoes_random <- predict(random_final, data = conj_teste)

# Probabilidades da classe positiva 
probs_random <- factor(ifelse(predicoes_random$predictions[,2] > 0.5, 1, 0))

# Matriz de confusão e sensibilidade
confusion_random <- caret::confusionMatrix(data=probs_random, reference = as.factor(conj_teste$target), positive='1')

confusion_random
```
Obtendo a AUC:
```{r}
probs_numeric <- as.numeric(as.character(predicoes_random$predictions[,2]))
labels_numeric <- as.numeric(as.character(conj_teste$target))

roc_obj <- roc(labels_numeric, probs_numeric)
auc_random <- auc(roc_obj)

auc_random
```


Adicionando resultados no tibble de comparação


```{r}
tb_metricas$AUC[tb_metricas$modelo == "Random Forest"] = auc_random
tb_metricas
```

### Regressão Logistica

Para a realizar a regressão logistica, primeiro será necessario criar as matrizes de entrada.

```{r}
reg_logistica <- glm(target ~ ., data = conj_treinamento, family = "binomial")

reg_logistica
```

Cálculando as probabilidades

```{r}
prob_log <- predict(reg_logistica, conj_teste, type = "response")
```

Matriz de confusão

```{r}

class_log <- ifelse(prob_log > 0.5, 1, 0) 

# Matriz de confusão e sensibilidade
confusion_reglog <- caret::confusionMatrix(data=factor(class_log), reference = as.factor(conj_teste$target), positive='1')

confusion_reglog
```



Cálculando métrica AUC

```{r}
roc_log <- roc(conj_teste$target, prob_log)

roc_log
```

Adicionando AUC para a tabela de resultados

```{r}
tb_metricas$AUC[tb_metricas$modelo == "Regressão Logistica"] = roc_log$auc

tb_metricas
```



### Ridge

Para utilizar o pacote glmnet, primeiro é necessario realizar um tratamento nos conjuntos de teste e treinamento.

```{r}
conj_treinamento_matrix <- model.matrix(target~., data = conj_treinamento)[, -1]

conj_teste_matrix <- model.matrix(target~. , data = conj_teste)[, -1]
```

Treinando o modelo

```{r}
ridge <- glmnet(conj_treinamento_matrix, conj_treinamento$target, alpha = 0, family = "binomial")

summary(ridge)
```

Realizando validação cruzada para otimizar lambda

```{r}
cv.ridge <- cv.glmnet(conj_treinamento_matrix, conj_treinamento$target, alpha = 0, family = "binomial")
best_lambda <- cv.ridge$lambda.min 
```

```{r}
best_lambda
```

Ajustando modelo e realizando as predições
```{r}
predicoes_prob <- predict(ridge, s = best_lambda, newx = conj_teste_matrix, type = "response")

predicoes_class <- ifelse(predicoes_prob > 0.5, 1, 0) 
```

Cálculando a matriz de confusão
```{r}
# Matriz de confusão
confusion_ridge <- caret::confusionMatrix(data=factor(predicoes_class), reference = as.factor(conj_teste$target), positive='1')

confusion_ridge
```

Obtendo a métrica AUC:

```{r}
roc_obj <- roc(conj_teste$target, predicoes_prob)  # Crie o objeto ROC
auc(roc_obj)  # Calcule a AUC
```
Adicionando resposta em tabela de comparação

```{r}
tb_metricas$AUC[tb_metricas$modelo == "Ridge"] = auc(roc_obj)

tb_metricas
```


### XGBoost

Treinando o modelo e obtendo a métrica AUC

```{r}
tr_sparse_matrix <- sparse.model.matrix(target ~ ., data = conj_treinamento)[,-1]

d_tr <- xgb.DMatrix(label = as.numeric(as.character(conj_treinamento$target)), 
                    data = tr_sparse_matrix)

xgb_model <- xgboost(data = d_tr, nrounds = 500, max_depth = 4, 
                      eta = 0.1, nthread = 3, verbose = FALSE,
                      objective = "binary:logistic")

test_sparse_matrix <- sparse.model.matrix(target ~ ., data = conj_teste)[,-1]

d_test <- xgb.DMatrix(label = conj_teste$target, data = test_sparse_matrix)

pred_xgb <- predict(xgb_model, d_test)

desempenho <- tibble(prob = pred_xgb, 
                     classes = conj_teste$target, 
                     metodo = "xgboost")

xgb_auc <- desempenho %>% 
  mutate(classes = factor(classes)) %>% 
  group_by(metodo) %>% 
  roc_auc(classes, prob, event_level = "second") %>% 
  arrange(desc(.estimate))

```

Otimizando os hiperparâmetros

```{r}
set.seed(234)

hiperparametros <- crossing(eta = c(.01, .1), 
                            nrounds = c(250, 750), 
                            max_depth = c(1, 4, 8))




ajusta_bst <- function(eta, nrounds, max_depth) {
  
  
  fit <- xgb.train(data = d_tr, nrounds = nrounds, max_depth = max_depth, eta = eta, 
                   nthread = 10, verbose = FALSE, objective = "binary:logistic")
  
  pred_xgb <- predict(fit, d_test)

  desempenho <- tibble(prob = pred_xgb, 
                     classes = conj_teste$target, 
                     metodo = "xgboost")

  return(desempenho %>% 
  mutate(classes = factor(classes)) %>% 
  group_by(metodo) %>% 
  roc_auc(classes, prob, event_level = "second") %>% 
  arrange(desc(.estimate)) %>% pull(.estimate))
  
  
}


resultados <- 
  crossing(hiperparametros) %>% 
  mutate(auc = pmap_dbl(list(eta, nrounds, max_depth), ajusta_bst))

resultados %>% arrange(desc(auc))
```

Aplicando melhores hiperparâmetros

```{r}
xgb_model <- xgboost(data = d_tr, nrounds = 250, max_depth = 4, 
                      eta = 0.1, nthread = 10, verbose = FALSE,
                      objective = "binary:logistic")

pred_xgb <- predict(xgb_model, d_test)

desempenho <- tibble(prob = pred_xgb, 
                     classes = conj_teste$target, 
                     metodo = "xgboost")

xgb_auc <- desempenho %>% 
  mutate(classes = factor(classes)) %>% 
  group_by(metodo) %>% 
  roc_auc(classes, prob, event_level = "second") %>% 
  arrange(desc(.estimate))

xgb_auc
```
Adicionando resposta em tabela de comparação

```{r}
tb_metricas$AUC[tb_metricas$modelo == "XGboost"] = xgb_auc$.estimate

tb_metricas

```

### Selecionamos o modelo com o melhor AUC, que foi o XGBoost.

Agora iremos entender a importância das variáveis e selecionar o melhor threshold.

Plotando feature importance

```{r}
importancia <- xgb.importance(model = xgb_model)

xgb.plot.importance(importancia, rel_to_first = TRUE, xlab = "Relative Importance", top_n = 10)
```

Buscando todos os thresholds possíveis

```{r}
desempenho %>% 
  mutate(classes = factor(classes)) %>% 
  group_by(metodo) %>% 
  roc_curve(classes, prob, event_level = "second")

```

Selecionando o melhor threshold e plotando a matriz de confusão final

```{r}
df_threshold <- desempenho %>% 
  mutate(classes = factor(classes)) %>% 
  group_by(metodo) %>% 
  roc_curve(classes, prob, event_level = "second")

gmeans <- sqrt(df_threshold$sensitivity * df_threshold$specificity)

(ideal_threshold <- df_threshold[which.max(gmeans),]$.threshold)

pred_ <- as.factor(ifelse(predict(xgb_model, d_test, type="response")>=ideal_threshold,"1","0"))

caret::confusionMatrix(data=pred_, reference = factor(conj_teste$target), positive='1')
```










